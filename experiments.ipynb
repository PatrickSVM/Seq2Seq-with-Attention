{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Machine Translation with Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets as lists\n",
    "with open(\"./data/europarl-v7.de-en.de\") as file:\n",
    "    ger = [line.rstrip() for line in file]\n",
    "with open(\"./data/europarl-v7.de-en.en\") as file:\n",
    "    eng = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920209\n"
     ]
    }
   ],
   "source": [
    "print(len(eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english words: 47882343\n",
      "Number of german words: 44614285\n"
     ]
    }
   ],
   "source": [
    "words = 0\n",
    "for sent in eng:\n",
    "    words += len(sent.split())\n",
    "print(f\"Number of english words: {words}\")\n",
    "\n",
    "words = 0\n",
    "for sent in ger:\n",
    "    words += len(sent.split())\n",
    "print(f\"Number of german words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - use spacy\n",
    "import spacy\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    \"\"\"\n",
    "    Take german sentence and tokenize it using spacy. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    \"\"\"\n",
    "    Take english sentence and tokenize it using spacy. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "src_field = Field(init_token = '<sos>', \n",
    "            eos_token = '<eos>',\n",
    "            pad_token='<pad>', \n",
    "            unk_token='<unk>',\n",
    "            lower = True, \n",
    "            include_lengths = True,\n",
    "            sequential=True,\n",
    "            batch_first=True)\n",
    "\n",
    "trg_field = Field(init_token = '<sos>', \n",
    "            eos_token = '<eos>',\n",
    "            pad_token='<pad>', \n",
    "            unk_token='<unk>',\n",
    "            lower = True, \n",
    "            include_lengths = True,\n",
    "            sequential=True,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "ger_token = [tokenize_ger(sent) for sent in ger]\n",
    "eng_token = [tokenize_eng(sent) for sent in eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size 32K\n",
    "# https://jlibovicky.github.io/2021/07/24/MT-Weekly-The-Wisdom-of-the-WMT-Crowd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 32000\n",
    "src_field.build_vocab(ger_token, min_freq=2, max_size=max_vocab_size)\n",
    "trg_field.build_vocab(eng_token,  min_freq=2, max_size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397884\n",
      "32004\n",
      "32004\n"
     ]
    }
   ],
   "source": [
    "# Frequency hodls all frequencies\n",
    "print(len(SRC.vocab.freqs))\n",
    "\n",
    "# ITOS/\n",
    "print(len(SRC.vocab.stoi))\n",
    "print(len(SRC.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TextDtaatset(Dataset):\n",
    "\n",
    "  def __init__(self, data):\n",
    "    self.text = data\n",
    "    \n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.text)\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    return self.text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_dataset = TextDtaatset(data=ger_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define iterator\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "iterator = BucketIterator.splits(datasets=ger_dataset, batch_size=2, sort_key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "with open('./data/train_de') as src, open('./data/train_en') as tgt:\n",
    "    with open('./data/train.csv','w') as file:\n",
    "        for src_sentence, tgt_sentence in zip(src, tgt):\n",
    "            line = f'{src_sentence.rstrip()} , {tgt_sentence.rstrip()}'\n",
    "            file.write(line)\n",
    "            file.write('\\n')\n",
    "\n",
    "with open('./data/val_de') as src, open('./data/val_en') as tgt:\n",
    "    with open('./data/val.csv','w') as file:\n",
    "        for src_sentence, tgt_sentence in zip(src, tgt):\n",
    "            line = f'{src_sentence.rstrip()} , {tgt_sentence.rstrip()}'\n",
    "            file.write(line)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_obj, valid_obj) = TabularDataset.splits(\n",
    "  path=\"\",\n",
    "  train='./data/val.csv',\n",
    "  validation='./data/val.csv',\n",
    "  format='csv',\n",
    "  fields=[('src',src_field ), ('trg', trg_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_field' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mseq2seq_attention\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuild_dataloaders\u001b[39;00m \u001b[39mimport\u001b[39;00m build_vocab\n\u001b[0;32m----> 2\u001b[0m build_vocab(train_set\u001b[39m=\u001b[39;49mtrain_obj)\n",
      "File \u001b[0;32m~/Documents/Python_projects/Seq2Seq-with-Attention/seq2seq_attention/build_dataloaders.py:79\u001b[0m, in \u001b[0;36mbuild_vocab\u001b[0;34m(train_set, min_freq, max_vocab_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mglobal\u001b[39;00m src_field\n\u001b[1;32m     78\u001b[0m \u001b[39mglobal\u001b[39;00m trg_field \n\u001b[0;32m---> 79\u001b[0m src_field\u001b[39m.\u001b[39mbuild_vocab(train_set, min_freq\u001b[39m=\u001b[39mmin_freq, max_size\u001b[39m=\u001b[39mmax_vocab_size)\n\u001b[1;32m     80\u001b[0m trg_field\u001b[39m.\u001b[39mbuild_vocab(train_set,  min_freq\u001b[39m=\u001b[39mmin_freq, max_size\u001b[39m=\u001b[39mmax_vocab_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src_field' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from seq2seq_attention.build_dataloaders import build_vocab\n",
    "build_vocab(train_set=train_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'build_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[247], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m src_field \u001b[39m=\u001b[39m src_field\u001b[39m.\u001b[39;49mbuild_vocab(train_obj, min_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_size\u001b[39m=\u001b[39mmax_vocab_size)\n\u001b[1;32m      2\u001b[0m trg_field \u001b[39m=\u001b[39m trg_field\u001b[39m.\u001b[39mbuild_vocab(train_obj,  min_freq\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_size\u001b[39m=\u001b[39mmax_vocab_size)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'build_vocab'"
     ]
    }
   ],
   "source": [
    "src_field.build_vocab(train_obj, min_freq=1, max_size=max_vocab_size)\n",
    "trg_field.build_vocab(train_obj,  min_freq=1, max_size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(\n",
    "  dataset=train_obj,\n",
    "  batch_size = 2,\n",
    "  sort_key=lambda x: len(x.src),\n",
    "  shuffle=True,\n",
    "  device=\"cpu\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 2,  4,  6,  3,  1,  1],\n",
      "        [ 2, 14,  8,  7, 10,  3]]), tensor([4, 6]))\n",
      "(tensor([[ 2,  5,  9,  3,  1],\n",
      "        [ 2,  7,  4, 10,  3]]), tensor([4, 5]))\n"
     ]
    }
   ],
   "source": [
    "example=next(iter(train_iter))\n",
    "src = example.src\n",
    "trg = example.trg\n",
    "print(src)\n",
    "print(trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>\n",
      "bestens\n",
      "danke\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "\n",
      "<sos>\n",
      "fine\n",
      "thanks\n",
      "<eos>\n",
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "# itos is list of token strings with their idx \n",
    "for i in src[0][0]:\n",
    "    print(src_field.vocab.itos[i])\n",
    "print()\n",
    "for i in trg[0][0]:\n",
    "    print(trg_field.vocab.itos[i])\n",
    "\n",
    "# The second element in the tuple is the real length that we pass to the packed_seq!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional GRU\n",
    "import torch.nn as nn\n",
    "emb = nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
    "gru = nn.GRU(input_size=10, hidden_size=4, num_layers=1,  bidirectional = True, batch_first=True, bias=True,)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input = torch.tensor([[1,2], [1,2], [0,1]])\n",
    "embed = emb(input)\n",
    "all, last = gru(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenaeted version of the hdiden states \n",
    "all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bidir, batch, hidden_dim) two hidden states for all directions, \n",
    "last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
