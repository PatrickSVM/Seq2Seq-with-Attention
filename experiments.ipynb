{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Machine Translation with Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets as lists\n",
    "with open(\"./data/europarl-v7.de-en.de\") as file:\n",
    "    ger = [line.rstrip() for line in file]\n",
    "with open(\"./data/europarl-v7.de-en.en\") as file:\n",
    "    eng = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920209\n"
     ]
    }
   ],
   "source": [
    "print(len(eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english words: 47882343\n",
      "Number of german words: 44614285\n"
     ]
    }
   ],
   "source": [
    "words = 0\n",
    "for sent in eng:\n",
    "    words += len(sent.split())\n",
    "print(f\"Number of english words: {words}\")\n",
    "\n",
    "words = 0\n",
    "for sent in ger:\n",
    "    words += len(sent.split())\n",
    "print(f\"Number of german words: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - use spacy\n",
    "import spacy\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    \"\"\"\n",
    "    Take german sentence and tokenize it using spacy. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    \"\"\"\n",
    "    Take english sentence and tokenize it using spacy. \n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "src_field = Field(init_token = '<sos>', \n",
    "            eos_token = '<eos>',\n",
    "            pad_token='<pad>', \n",
    "            unk_token='<unk>',\n",
    "            lower = True, \n",
    "            include_lengths = True,\n",
    "            sequential=True,\n",
    "            batch_first=True)\n",
    "\n",
    "trg_field = Field(init_token = '<sos>', \n",
    "            eos_token = '<eos>',\n",
    "            pad_token='<pad>', \n",
    "            unk_token='<unk>',\n",
    "            lower = True, \n",
    "            include_lengths = True,\n",
    "            sequential=True,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "ger_token = [tokenize_ger(sent) for sent in ger]\n",
    "eng_token = [tokenize_eng(sent) for sent in eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab size 32K\n",
    "# https://jlibovicky.github.io/2021/07/24/MT-Weekly-The-Wisdom-of-the-WMT-Crowd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 32000\n",
    "src_field.build_vocab(ger_token, min_freq=2, max_size=max_vocab_size)\n",
    "trg_field.build_vocab(eng_token,  min_freq=2, max_size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397884\n",
      "32004\n",
      "32004\n"
     ]
    }
   ],
   "source": [
    "# Frequency hodls all frequencies\n",
    "print(len(SRC.vocab.freqs))\n",
    "\n",
    "# ITOS/\n",
    "print(len(SRC.vocab.stoi))\n",
    "print(len(SRC.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TextDtaatset(Dataset):\n",
    "\n",
    "  def __init__(self, data):\n",
    "    self.text = data\n",
    "    \n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.text)\n",
    "\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "\n",
    "    return self.text[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_dataset = TextDtaatset(data=ger_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define iterator\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "iterator = BucketIterator.splits(datasets=ger_dataset, batch_size=2, sort_key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "with open('./data/train_de') as src, open('./data/train_en') as tgt:\n",
    "    with open('./data/train.csv','w') as file:\n",
    "        for src_sentence, tgt_sentence in zip(src, tgt):\n",
    "            line = f'{src_sentence.rstrip()} , {tgt_sentence.rstrip()}'\n",
    "            file.write(line)\n",
    "            file.write('\\n')\n",
    "\n",
    "with open('./data/val_de') as src, open('./data/val_en') as tgt:\n",
    "    with open('./data/val.csv','w') as file:\n",
    "        for src_sentence, tgt_sentence in zip(src, tgt):\n",
    "            line = f'{src_sentence.rstrip()} , {tgt_sentence.rstrip()}'\n",
    "            file.write(line)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_obj, valid_obj) = TabularDataset.splits(\n",
    "  path=\"\",\n",
    "  train='./data/val.csv',\n",
    "  validation='./data/val.csv',\n",
    "  format='csv',\n",
    "  fields=[('src',src_field ), ('trg', trg_field)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_field.build_vocab(train_obj, min_freq=1, max_size=max_vocab_size)\n",
    "trg_field.build_vocab(train_obj,  min_freq=1, max_size=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(\n",
    "  dataset=train_obj,\n",
    "  batch_size = 2,\n",
    "  sort_key=lambda x: len(x.src),\n",
    "  shuffle=True,\n",
    "  device=\"cpu\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 2,  4,  6,  3,  1,  1],\n",
      "        [ 2, 14,  8,  7, 10,  3]]), tensor([4, 6]))\n",
      "(tensor([[ 2,  5,  9,  3,  1],\n",
      "        [ 2,  7,  4, 10,  3]]), tensor([4, 5]))\n"
     ]
    }
   ],
   "source": [
    "example=next(iter(train_iter))\n",
    "src = example.src\n",
    "trg = example.trg\n",
    "print(src)\n",
    "print(trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>\n",
      "bestens\n",
      "danke\n",
      "<eos>\n",
      "<pad>\n",
      "<pad>\n",
      "\n",
      "<sos>\n",
      "fine\n",
      "thanks\n",
      "<eos>\n",
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "# itos is list of token strings with their idx \n",
    "for i in src[0][0]:\n",
    "    print(src_field.vocab.itos[i])\n",
    "print()\n",
    "for i in trg[0][0]:\n",
    "    print(trg_field.vocab.itos[i])\n",
    "\n",
    "# The second element in the tuple is the real length that we pass to the packed_seq!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
